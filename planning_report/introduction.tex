\section{Introduction}

Localization is a crucial part of navigation systems. Automotive
systems have been able to rely on \gls{GNSS} positioning for driver
assistance, where typical position error is are in the
order of 10 m \cite{4770175}. This magnitude of error is acceptable
when the purpose of the position estimate is to function as driver assistance.
However, a prerequisite for autonomous cars is the ability of the car
system to robustly localize in many types of environments and weather
conditions. For instance, spontaneous change lane can occur if the
lane width is in the same magnitude as the positioning
error. Additionally, the environments can include cities with
high-rise buildings and tunnels, where the performance of \gls{GNSS}
systems could degrade up to the point of not functioning at all.

Thus, the autonomous car needs additional types of sensors for
localization, then solely a \gls{GNSS} receiver. The types of sensors
that have been used for cars are: \gls{LIDAR}, cameras, \gls{IMU}, and
radio receivers. However, many of these systems requires a predefined map of the
environment, or identifiable landmarks, for the localization system to
determine its position in the predefined map, or relative position
compared to the landmarks. For instance, if the location of a stop
sign was known a priori to the car, the car could measure angle and
distance to the sign in its vicinity. If a \gls{LIDAR} is used, the
typical measurement error is in the order of centimeters, which thus
would be the position error of the car. However, a priori knowledge of the location of
landmarks, to such a degree to render the position error of the car
acceptable, is rarely available. Thus, there is an additional problem
of mapping the environment.

\subsection{Goal}

This project aims at producing an algorithm that provides a solution to the localization problem of the car. The problem is restricted to perform the mapping off-line and the localization on-line. However, the mapping should be done with the same sensors used in the localization and come from normal usage of a car operated by a human.  More
specifically:
\begin{itemize}
\item Use sensor data from normal operation of vehicles, a type of crowd sourcing, to estimate positions of landmarks, i.e. create maps, suitable for localization.

\item Given a map, a vehicle should be able to locate itself in that map using the same type of sensors.
\end{itemize}
The scenario to be considered is:
\begin{enumerate}
\item Install sensors on multiple cars, which operate in a normal fashion with human drivers.
\item Let the sensors record and store data.
\item With the recorded data create a global map off-line.
\item A car entering an area that has been mapped by previous car can
  thus localize itself in this map.
\item New cars can send new sensor measurements to update the global
  map.
\end{enumerate}
The global map refers to an unique map that is shared between
vehicles. With such a system, questions like ``When is the traffic sign
coming?'' or  ``How far is this tunnel?'' can be answered.

The quality of a landmark are two fold: the error in the estimated position of landmark and the likelihood of detecting the landmark from sensor data. If the position of a landmark can be estimated with high accuracy from some car, but cannot be readily detected by other cars, it is questionable if that type of landmark should be used. Also, the landmark maybe only be detectable during certain conditions that may not always be true. For instance, an advertisement sign could only be detected when it is in the vicinity of the road, however the sign might be replaced after a while. Thus, the extracted landmarks should be detectable in many types of
weathers and lighting conditions, to render the system robust.


\subsection{SLAM}

The problem of jointly estimating the position of the vehicle and the landmarks
is referred to as the \gls{SLAM} problem
\cite{DBLP:journals/corr/CadenaCCLSN0L16}. The landmark can represent any
well-defined point in the world, that can be detected by sensors. The landmarks
can differ depending on what type of sensor that is used. For instance, a radio
transmitter can receive signals from the base station, and thus the relative
distance between the transmitter and the base-station can be measured. However,
from radio measurements, information about the location of a stop sign cannot
be extracted, which could be provided by a camera sensor. Thus, different types
of sensor modalities can yield different types of landmarks, but they could
also be coupled, e.g. the location of a stop sign can be measured from a
camera, but also from a \gls{LIDAR}. In the sections below, some
sensor types are described.

\subsection{Radio SLAM}

\gls{GNSS} signals may not always be available. With the emergence of 5G
standard, the project would also consider \gls{SLAM} based on radio receiver
measurements. Here the measurements would correspond to signal strength and/or
angle of arrival of the radio signal from the base-station to the radio
receiver. The landmarks in this sensor modality is then the location of the
base-stations. The typical localization measurements performed the in radio receivers
are angular and distance information to the base-stations. Typically,
these radio stations already have a determined position, which would
reduce the \gls{SLAM} problem by omitting the estimation of the
positions of the base-stations.

\subsection{Visual SLAM}

For the visual based \gls{SLAM}, different camera set-ups, e.g., monocular,
stereo or RGB-D
cameras, require different algorithms for estimating the motion of
the camera. In camera based \gls{SLAM}, the landmarks corresponds to features
that are extracted from the image. There are different methods that extract
features from an image, e.g. \gls{SIFT}\cite{Lowe:1999:ORL:850924.851523},
\gls{SURF}\cite{Bay:2008:SRF:1370312.1370556} and
\gls{ORB}\cite{Rublee:2011:OEA:2355573.2356268}.

In this project, using a stereo camera is a more feasible option
because a RGB-D camera naturally does not work well in outdoor scenario
and single camera potentially will suffer from the lack of estimation
of true scale \cite{Royer2007}, if no reference is given.

For stereo visual \gls{SLAM}, there are several modern systems
which have been extensively verified in different
benchmarks. \gls{ORB}-\gls{SLAM}\cite{DBLP:journals/corr/Mur-ArtalT16a} is one
of the modern sparse visual \gls{SLAM}
system, it is based on key-point feature matching and sparse bundle
adjustment for estimating the motion of camera. Stereo \gls{LSD}-\gls{SLAM}\cite{7353631} is
a direct method which is feature free, the tracker is directly
optimizing the photometric error of  high gradient regions in the
images. \gls{SVO}\cite{7782863} is a combination of both, it firstly detects sparse
features and then uses small image patches around them to perform
direct tracking. The tracked points are then input to a sparse bundle
adjustment back-end.

\subsection{LiDAR SLAM}

As opposed to the camera based \gls{SLAM}, \gls{LIDAR}s capable
of generating dense and accurate depth map with relatively long range
outdoor\cite{7487258}. It is usually based on the direct alignment of point cloud
using classical iterative closest point algorithm or its variants.

\subsection{Inertial Navigation}

When constructing \gls{SLAM} systems, it is common to include \gls{IMU} to
provide an inertial navigation solution. Since it is low
cost and has high data rate (hundreds of Hz), it can estimate the motion of the
target in-between other types of measurements. For instance, the \gls{IMU} can
be used to compensate for the scale drift that can occur in the camera or
\gls{LIDAR} estimation of the motion.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
